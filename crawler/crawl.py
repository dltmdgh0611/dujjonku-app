"""
ë‘ì«€ì¿ ë¥¼ ì°¾ì•„ë¼ í¬ë¡¤ëŸ¬
10ë¶„ë§ˆë‹¤ GitHub Actionsì—ì„œ ìë™ ì‹¤í–‰
"""

import requests
import json
import re
from datetime import datetime, timezone, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

URL = "https://www.dubaicookiemap.com"
OUTPUT = "public/stores.json"
KST = timezone(timedelta(hours=9))

# ëª¨ë°”ì¼ User-Agent (naver.me ë¦¬ì¡¸ë¸Œìš©)
MOBILE_UA = 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1'

def convert_to_mobile_url(url, retries=2):
    """URLì„ ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜ (ì¬ì‹œë„ í¬í•¨)"""
    if not url:
        return url
    
    # ì´ë¯¸ ëª¨ë°”ì¼ URLì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if 'm.place.naver.com' in url or 'm.map.naver.com' in url:
        return url
    
    # PC URLì„ ëª¨ë°”ì¼ë¡œ ë³€í™˜
    if 'place.naver.com' in url:
        return url.replace('place.naver.com', 'm.place.naver.com')
    if 'map.naver.com' in url:
        return url.replace('map.naver.com', 'm.map.naver.com')
    
    # naver.me ë‹¨ì¶• URLì€ ë¦¬ì¡¸ë¸Œí•´ì„œ ëª¨ë°”ì¼ URLë¡œ ë³€í™˜
    if 'naver.me' in url:
        for attempt in range(retries):
            try:
                # HEAD ìš”ì²­ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ìœ„ì¹˜ í™•ì¸ (ëª¨ë°”ì¼ UA ì‚¬ìš©)
                res = requests.head(url, allow_redirects=True, timeout=8, headers={
                    'User-Agent': MOBILE_UA
                })
                final_url = res.url
                # ìµœì¢… URLì„ ëª¨ë°”ì¼ë¡œ ë³€í™˜
                if 'place.naver.com' in final_url and 'm.place.naver.com' not in final_url:
                    final_url = final_url.replace('place.naver.com', 'm.place.naver.com')
                if 'map.naver.com' in final_url and 'm.map.naver.com' not in final_url:
                    final_url = final_url.replace('map.naver.com', 'm.map.naver.com')
                return final_url
            except Exception:
                if attempt < retries - 1:
                    continue  # ì¬ì‹œë„
                # ìµœì¢… ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ URL ë°˜í™˜
                return url
    
    return url

def convert_urls_batch(cafes):
    """ëª¨ë“  ì¹´í˜ URLì„ ë³‘ë ¬ë¡œ ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜"""
    print("ğŸ”„ URLì„ ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    
    results = {}
    with ThreadPoolExecutor(max_workers=20) as executor:
        future_to_idx = {
            executor.submit(convert_to_mobile_url, cafe.get('naver_place_url', '')): idx 
            for idx, cafe in enumerate(cafes)
        }
        
        done_count = 0
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                results[idx] = future.result()
            except Exception:
                results[idx] = cafes[idx].get('naver_place_url', '')
            
            done_count += 1
            if done_count % 100 == 0:
                print(f"  ... {done_count}/{len(cafes)} ì™„ë£Œ")
    
    # ê²°ê³¼ ì ìš©
    for idx, url in results.items():
        cafes[idx]['naver_place_url'] = url
    
    print(f"âœ… URL ë³€í™˜ ì™„ë£Œ")
    return cafes

def crawl():
    print("ğŸª í¬ë¡¤ë§ ì‹œì‘...")
    
    try:
        res = requests.get(URL, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        html = res.text
        print(f"âœ… HTML ë¡œë“œ ({len(html):,} bytes)")
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {e}")
        return None
    
    # Next.js self.__next_f.push ì•ˆì— cafes ë°ì´í„° ì°¾ê¸°
    print("ğŸ” self.__next_f.push ì•ˆì—ì„œ cafes ì°¾ê¸°...")
    
    # ë°©ë²• 1: ì •ê·œì‹ìœ¼ë¡œ cafes ë°°ì—´ ì¶”ì¶œ (ë” ì•ˆì •ì )
    # ì´ìŠ¤ì¼€ì´í”„ëœ í˜•íƒœ: \"cafes\":[...]
    pattern = r'\\"cafes\\":\s*(\[.*?\])\s*(?:,\s*\\"|\})'
    match = re.search(pattern, html, re.DOTALL)
    
    if not match:
        # ì´ìŠ¤ì¼€ì´í”„ ì—†ëŠ” í˜•íƒœë¡œë„ ì‹œë„
        pattern = r'"cafes":\s*(\[.*?\])\s*(?:,\s*"|\})'
        match = re.search(pattern, html, re.DOTALL)
    
    if not match:
        # ë°©ë²• 2: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback (ê°œì„ ëœ íŒŒì‹±)
        json_str = extract_cafes_fallback(html)
        if not json_str:
            print("âŒ cafes ì—†ìŒ")
            return None
    else:
        json_str = match.group(1)
        print(f"âœ… cafes ë°œê²¬! (ì •ê·œì‹ ë§¤ì¹­)")
    
    # ì´ìŠ¤ì¼€ì´í”„ ì œê±°
    json_str = json_str.replace('\\"', '"')
    json_str = json_str.replace('\\\\', '\\')
    
    try:
        cafes = json.loads(json_str)
        print(f"âœ… {len(cafes)}ê°œ ì¹´í˜ ì¶”ì¶œ")
        
        # URLì„ ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜
        cafes = convert_urls_batch(cafes)
        
        return process_cafes(cafes)
    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        print(f"ğŸ“„ JSON ìƒ˜í”Œ (ì²˜ìŒ 200ì): {json_str[:200]}")
        return None

def extract_cafes_fallback(html):
    """ê¸°ì¡´ ë°©ì‹ì˜ ê°œì„ ëœ ë²„ì „ - ë¬¸ìì—´ ë‚´ë¶€ ê³ ë ¤"""
    # ì´ìŠ¤ì¼€ì´í”„ëœ í˜•íƒœë¡œ ì°¾ê¸°
    marker = '\\"cafes\\":['
    start = html.find(marker)
    is_escaped = True
    
    if start == -1:
        marker = '"cafes":['
        start = html.find(marker)
        is_escaped = False
    
    if start == -1:
        return None
    
    print(f"âœ… cafes ë°œê²¬! ìœ„ì¹˜: {start}, ì´ìŠ¤ì¼€ì´í”„: {is_escaped}")
    
    # cafes ë°°ì—´ ì‹œì‘ ìœ„ì¹˜
    arr_start = start + len(marker) - 1
    
    # ë¬¸ìì—´ ë‚´ë¶€ì¸ì§€ ì¶”ì í•˜ë©´ì„œ ë°°ì—´ ë ì°¾ê¸°
    depth = 0
    in_string = False
    i = arr_start
    
    while i < len(html):
        c = html[i]
        
        # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ì ê±´ë„ˆë›°ê¸°
        if is_escaped:
            if html[i:i+2] == '\\"':
                i += 2
                continue
            if html[i:i+2] == '\\\\':
                i += 2
                continue
        else:
            if i > 0 and html[i-1] == '\\' and c == '"':
                i += 1
                continue
        
        # ë¬¸ìì—´ ì‹œì‘/ë ì¶”ì 
        if c == '"':
            in_string = not in_string
        
        # ë¬¸ìì—´ ë°–ì—ì„œë§Œ ê¹Šì´ ì¶”ì 
        if not in_string:
            if c == '[':
                depth += 1
            elif c == ']':
                depth -= 1
                if depth == 0:
                    return html[arr_start:i+1]
        
        i += 1
    
    return None

def get_stock_value(cafe):
    """stock_status ë˜ëŠ” stock_countì—ì„œ ì¬ê³  ê°’ ì¶”ì¶œ"""
    # stock_countê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ìˆ«ì)
    if 'stock_count' in cafe:
        return cafe.get('stock_count', 0)
    
    # stock_statusê°€ ìˆìœ¼ë©´ ë³€í™˜ (ë¬¸ìì—´)
    status = cafe.get('stock_status', '')
    if status == 'SOLDOUT' or status == 'sold_out' or status == '':
        return 0
    elif status == 'IN_STOCK' or status == 'in_stock' or status == 'AVAILABLE':
        return 1
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœëŠ” ì¬ê³  ìˆìŒìœ¼ë¡œ ì²˜ë¦¬
        return 1 if status else 0

def process_cafes(cafes):
    """ì¹´í˜ ë°ì´í„° ê°€ê³µ ë° ì €ì¥"""
    # ì •ë¦¬ (í•„ìš”í•œ í•„ë“œë§Œ, ìš©ëŸ‰ ìµœì†Œí™”)
    data = []
    for c in cafes:
        stock = get_stock_value(c)
        data.append({
            "n": c.get("name", ""),
            "a": c.get("address", ""),
            "y": c.get("lat", 0),
            "x": c.get("lng", 0),
            "s": stock,
            "u": c.get("naver_place_url", "")
        })
    
    # ì¬ê³  ìˆëŠ” ê³³ ë¨¼ì €
    data.sort(key=lambda x: (x['s'] == 0, -x['s']))
    
    available = sum(1 for d in data if d['s'] > 0)
    
    output = {
        "t": datetime.now(KST).strftime("%m/%d %H:%M"),
        "c": len(data),
        "a": available,
        "d": data
    }
    
    with open(OUTPUT, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, separators=(',', ':'))
    
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {len(data)}ê°œ (ì¬ê³ ìˆìŒ: {available})")
    return output

if __name__ == "__main__":
    crawl()
